---
title: "Support Vector Machines"
author: "Daniel Fuller"
date: "2024-09-23"
output:
      html_document:
        keep_md: true
---

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(sjPlot)
library(finalfit)
library(knitr)
library(gtsummary)
library(mlbench)
library(kernlab)
library(vip)
library(rsample)
library(tune)
library(recipes)
library(yardstick)
library(parsnip)
library(glmnet)
library(themis)
library(microbenchmark)
library(palmerpenguins)
```

# Support Vector Machines

## Research question and data

We are using an imputed (ie. no missing data) version of the CanPath student dataset [https://canpath.ca/student-dataset/](https://canpath.ca/student-dataset/). The nice thing about this dataset is that it's pretty big in terms of sample size, has lots of variables, and we can use it for free. 

Our research question is:  

- **Can we develop a model that will predict type 2 diabetes**

### Reading in data

Here are reading in data and getting organized to run our models. 

```{r}
data <- read_csv("mice_all_imp.csv")

data <- data %>% mutate_at(3, factor)
data <- data %>% mutate_at(5:6, factor)
data <- data %>% mutate_at(8:9, factor)
data <- data %>% mutate_at(12:12, factor)
data <- data %>% mutate_at(15:81, factor)
data <- data %>% mutate_at(83:93, factor)

table(data$DIS_DIAB_EVER)

data <- data %>%
	mutate(diabetes = case_when(
		DIS_DIAB_EVER == 0 ~ 0,
		DIS_DIAB_EVER == 1 ~ 1,
		DIS_DIAB_EVER == 2 ~ 0)) %>%
		mutate(diabetes = as.factor(diabetes))

table(data$DIS_DIAB_EVER, data$diabetes)

data$DIS_DIAB_EVER <- NULL
```

```{r}
data_continuous <- select(data, diabetes, 
                            PSE_ADULT_WRK_DURATION, 
                            PM_BMI_SR, 
                            PA_TOTAL_SHORT, 
                            SDC_HOUSEHOLD_CHILDREN_NB, 
                            SDC_HOUSEHOLD_ADULTS_NB, 
                            SDC_EDU_LEVEL_AGE, 
                            SDC_AGE_CALC, 
                            SDC_GENDER)

data <- select(data, diabetes, SDC_AGE_CALC, SDC_EDU_LEVEL, PM_BMI_SR, HS_GEN_HEALTH, WRK_FULL_TIME, SMK_CIG_EVER, SDC_INCOME, PA_TOTAL_SHORT, HS_ROUTINE_VISIT_EVER, PSE_ADULT_WRK_DURATION, DIS_RESP_SLEEP_APNEA_EVER, SDC_EDU_LEVEL_AGE, SDC_GENDER)
```

# Support Vector Machine

### Creating training and testing data

```{r}
set.seed(10)

#### Cross Validation Split
cv_split <- initial_validation_split(data_continuous, 
                            strata = diabetes, 
                            prop = c(0.70, 0.20))

# Create data frames for the two sets:
train_data <- training(cv_split)
table(train_data$diabetes)

test_data  <- testing(cv_split)
table(test_data$diabetes)
```

### V folds

```{r}
folds <- vfold_cv(training(cv_split), v = 3, strata = diabetes)
```

## Recipe

```{r}
svm_recipe <- 
  recipe(diabetes ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_normalize(all_numeric_predictors())
```

step_smotenc(diabetes, over_ratio = 0.9) %>%

### Model 

Here we use the tidy models to setup a model using `kernlab` and `classification` and we call the specific model we want to fit. 

* __cost__ (default: 1.0): A positive number for the cost of predicting a sample within or on the wrong side of the margin. 
Low cost, points that are misclassified are penalized less than with a higher cost. 
How much do we are about misclassification? 
* __margin__ (default: 0.1): Insensitivity margin, how wide a margin do we want to make to select the support vectors. (Does not apply in classification)
* __rbf_sigma__  rbf_sigma (no default â€“ estimated based on data) : A positive number for the radial basis function. Tune me!

```{r}
svm_model <- svm_rbf(cost = tune()) %>% 
  set_mode("classification") %>%
  set_engine("kernlab")
svm_model
```

### Workflow

```{r}
svm_grid <- grid_regular(
              #rbf_sigma(range = c(-2, 2)),
              cost(range = c(0, 20)),
              levels = 5  
            )
```


```{r}
roc_res <- metric_set(roc_auc) 
```

##### NOT RUN

```{r}
svm_workflow <- 
  workflow() %>% 
  add_model(svm_model) %>% 
  add_recipe(svm_recipe) %>% 
    tune_grid(resamples = folds,
              grid = svm_grid,
              metrics = roc_res,
                control = control_grid(save_pred = TRUE, 
                verbose = TRUE)) ## Edit for running live
```

```{r}
collect_metrics(svm_workflow)
```

### Pivot to toy data

Data example is a combination of the R4 Data Science tutorial and the Emil Hvitfeldt one. 

```{r}
penguins_df <- penguins %>%
  filter(!is.na(sex)) %>% # discarding NA obs
  select(-year, -island) # not useful

cv_split <- initial_validation_split(penguins_df, 
                            strata = sex, 
                            prop = c(0.70, 0.20))

# Create data frames for the two sets:
train_data <- training(cv_split)
table(train_data$sex)

test_data  <- testing(cv_split)
table(test_data$sex)

folds <- vfold_cv(training(cv_split), v = 5, strata = sex)

roc_res <- metric_set(roc_auc) # accuracy, a classification metric
```

```{r}
svm_recipe <- 
  recipe(sex ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_normalize(all_numeric_predictors())
```

```{r}
svm_model <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

```{r}
svm_grid <- grid_regular(
              rbf_sigma(range = c(-3, 3)),
              cost(range = c(1, 10)),
              levels = 5  
            )
```


```{r}
svm_workflow <- 
  workflow() %>% 
  add_model(svm_model) %>% 
  add_recipe(svm_recipe) %>% 
    tune_grid(resamples = folds,
              grid = svm_grid,
              metrics = roc_res,
                control = control_grid(save_pred = TRUE, 
                verbose = TRUE)) ## Edit for running live

metrics_svm <- collect_metrics(svm_workflow)
```

```{r}
svm_workflow %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, cost, rbf_sigma) %>%
  pivot_longer(cost:rbf_sigma,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```

```{r}
svm_best <- 
  svm_workflow %>% 
  select_best(metric = "roc_auc")

svm_best

svm_auc_fit <- 
  svm_workflow %>% 
  collect_predictions(parameters = svm_best) 
```


```{r}
final_model <- finalize_model(
                  svm_model,
                  svm_best
                )

final_model
```

### Variable Importance

```{}

```

### Final Model Fit

```{r}
final_svm_workflow <- workflow() %>%
                      add_recipe(svm_recipe) %>%
                      add_model(final_model)

final_results <- final_svm_workflow %>%
                    last_fit(cv_split)

final_results %>%
  collect_metrics()
```

# Resources

1. https://r4ds.github.io/bookclub-tmwr/svm-model-as-motivating-example.html
2. https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/09-support-vector-machines.html

## Session Info

```{r}
sessionInfo()
```


