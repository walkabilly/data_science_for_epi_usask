---
title: "Causal Quartet Data Work"
output:
      html_document:
        keep_md: true
---

### Required libraries

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggdag)
library(dagitty)
library(episensr) 
library(quartets)
library(reportRmd)
library(tidyverse)
library(gtsummary)
library(knitr)
library(epitools)

library(partykit)
library(randomForest)
library(neuralnet)
library(DALEX)
```

This data work class is based on the paper below and the [`quartets`](https://r-causal.github.io/quartets/#causal-quartet) R package. We will be working through some examples from the quartets package, specifically Anscombe's Quartet and the Causal Quartet. The examples are precisely the one's provided in the paper and in the package but I will add some explanation and discussion to the data work. 

  *  Lucy D’Agostino McGowan, Travis Gerke & Malcolm Barrett (2023) Causal Inference Is Not Just a Statistics Problem, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2023.2276446. 
  
## Anscombe's Quartet

From the `quartets` package and [wiki](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)

> The goal of the anscombe_quartet data set is to help drive home the point that visualizing your data is important. Francis Anscombe generated these four datasets to demonstrate that statistical summary measures alone cannot capture the full relationship between two variables (here, x and y). Anscombe emphasized the importance of visualizing data prior to calculating summary statistics.

* Dataset 1 has a linear relationship between x and y
* Dataset 2 has shows a nonlinear relationship between x and y
* Dataset 3 has a linear relationship between x and y with a single outlier
* Dataset 4 has shows no relationship between x and y with a single outlier that serves as a high-leverage point

### Load the data

```{r}
anscombe <- anscombe_quartet
```

### Summary statistics

Let's generate summary statistics for the data. Remember that this dataset was generated by a person to show why we should visualize our data. 

```{r}
anscombe_summary <- anscombe_quartet %>%
                      group_by(dataset) %>%
                      summarise(mean_x = mean(x),
                                var_x = var(x),
                                mean_y = mean(y),
                                var_y = var(y),
                                cor = cor(x, y)) 

kable(anscombe_summary, digits = 2)
```

Ok wow. If we look at the summary statistics they are all exactly the same. For each for the 4 datasets the mean and variance of x and y are the same, and the correlation between x and y is the same. This extends basically to all summary statistics, regression coefficients, whatever you want to calculate. 

### Visualize the data

Here we are doing a scatter plot between x and y grouped by dataset. 

```{r}
ggplot(anscombe_quartet, aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = "y ~ x") +
  facet_wrap(~dataset) + 
  theme_classic()
```

The data visualization shows that the datasets are very different in terms of the actual relationship between x and y. Anscombe's Quartet has been modernized into the [`DataSaurus Dozen`](https://cran.r-project.org/web/packages/datasauRus/index.html)... because why not. 

### Visualize the DataSaurus Dozen data

```{r}
ggplot(datasaurus_dozen, aes(x = x, y = y)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = "y ~ x") +
  facet_wrap(~dataset) +
  theme_classic()
```

That was fun! 

```{}
If you want you can calculate the summary statistics and correlations as a practice.
```

## Causal Quartet

Now will work on the causal quartet data. From the R package repo

> The goal of the causal_quartet data set is to help drive home the point that when presented with an exposure, outcome, and some measured factors, statistics alone, whether summary statistics or data visualizations, are not sufficient to determine the appropriate causal estimate. Additional information about the data generating mechanism is needed in order to draw the correct conclusions. 

Instead of 4 different relationships, as in the Anscombe's quartet data, we have 4 different types of potential mechanisms that might bias our results

1. Collider
2. Confounder
3. Mediator
4. M-Bias 

The causal quartet contains 100 simulated data points from each of the four mechanisms. This set of figures demonstrates that despite the very different data-generating mechanisms, there is no clear way to determine the “appropriate” way to model the effect of the exposure X and the outcome Y without additional information. The information below is combined from tables 1 and 2 from the paper.

| Technical term | Explanation | Correct causal model / analysis | True Association | 
| ----- | ----- | ----- | ----- |
| Collider | The exposure, X, causes a factor, Z, and the outcome, Y, causes a factor, Z. Adjusting for Z when estimating the effect of X on Y would yield a biased result. | Y ~ X | 1 | 
| Confounder | A factor, Z, causes both the exposure, X, and the outcome, Y. Failing to adjust for Z when estimating the effect of X on Y would yield a biased result. | Y ~ X ; Z | 0.5 | 
| Mediator | An exposure, X, causes a factor, Z, which causes the outcome, Y. Adjusting for Z when estimating the effect of X on Y would yield the direct effect, not adjusting for Z would yield the total effect of X on Y. The direct effect represents the relationship between X and Y independent of any mediator, while the total effect includes both the direct effect and any indirect effects mediated by the potential mediator. | Direct Effect: Y ~ X ; Z - Total Effect: Y ~ X | Direct effect: 0 - Total effect: 1 | 
|  M-Bias | here are two additional factors, U1 and U2. Both cause Z, U1 causes the exposure, X, and U2 causes the outcome, Y. Adjusting for Z when estimating the effect of X on Y will yield a biased result. | Y ~ X | 1 | 

### Visualize causal quartet

Let's visualize the data just to see what we are looking at. Remember that each dataset has a different data generating mechanism. I did not show that but it's in the paper. The the x,y association is different based on the presence of another variable. 

```{r}
ggplot(causal_quartet, aes(x = exposure, y = outcome)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = "y ~ x") +
  facet_wrap(~dataset) + 
  theme_classic()
```

Now let's look at the DAG for each of the 4 different data generating mechanism. I'm going to use the `daggity` package for this. 

## Collider

### Collider DAG

```{r}
collider_dag <- dagitty("dag{D -> C
                             E -> C
                             E -> D}")
tidy_dagitty(collider_dag)
ggdag(collider_dag, layout = "circle") + theme_dag() + coord_flip()
```

So based on what we know about the colliders so far should adjust or not adjust in this case? __NO__

Let's compare what happens when we do and do not adjust for the collider. 

##### Collider Regression

Get the data

```{r}
collider_data <- filter(causal_quartet, dataset == "(1) Collider")
```

Adjusted model

```{r}
collider_adjust <- lm(outcome ~ exposure + covariate, data = collider_data)
collider_adjust <- tbl_regression(collider_adjust)
collider_adjust %>% as_kable()
```

Unadjusted model 

```{r}
collider_unadjust <- lm(outcome ~ exposure, data = collider_data)
collider_unadjust <- tbl_regression(collider_unadjust) 
collider_unadjust %>% as_kable()
```

Both tables together

```{r}
collider_tble <- tbl_merge(list(collider_adjust, collider_unadjust))
collider_tble %>% as_kable()
```

So. When we adjust for the collider we get a biased estimate because the collider opens a path between the exposure and outcome through the collider. 


## Confounder

### Confounder DAG

```{r}
confounder_dag <- dagitty("dag{C -> D
                             C -> E
                             E -> D}")
tidy_dagitty(confounder_dag)

ggdag(confounder_dag, layout = "circle") + theme_dag() + coord_flip()
```

So based on what we know about confounders so far should adjust or not adjust in this case? __YES__

Let's compare what happens when we do and do not adjust for the confounder 

##### Confounder Regression

Get the data

```{r}
confounder_data <- filter(causal_quartet, dataset == "(2) Confounder")
```

Adjusted model

```{r}
confounder_adjust <- lm(outcome ~ exposure + covariate, data = confounder_data)
confounder_adjust <- tbl_regression(confounder_adjust)
confounder_adjust %>% as_kable()
```

Unadjusted model 

```{r}
confounder_unadjust <- lm(outcome ~ exposure, data = confounder_data)
confounder_unadjust <- tbl_regression(confounder_unadjust) 
confounder_unadjust %>% as_kable()
```

Both tables together

```{r}
confounder_tble <- tbl_merge(list(confounder_adjust, confounder_unadjust))
confounder_tble %>% as_kable()
```

So. When we adjust for the confounder we get the correct estimate because we block the path that exists between the confounder and the exposure and outcome. 

## Mediator

### Mediator DAG

```{r}
mediator_dag <- dagitty("dag{C -> D
                             E -> C
                             E -> D}")
tidy_dagitty(mediator_dag)
ggdag(mediator_dag, layout = "circle") + theme_dag() + coord_flip()
```

So based on what we know about mediators so far should adjust or not adjust in this case? __NO__

Let's compare what happens when we do and do not adjust for the mediator. 

##### Mediator Regression

Get the data

```{r}
mediator_data <- filter(causal_quartet, dataset == "(3) Mediator")
```

Adjusted model

```{r}
mediator_adjust <- lm(outcome ~ exposure + covariate, data = mediator_data)
mediator_adjust <- tbl_regression(mediator_adjust)
mediator_adjust %>% as_kable()
```

Unadjusted model 

```{r}
mediator_unadjust <- lm(outcome ~ exposure, data = mediator_data)
mediator_unadjust <- tbl_regression(mediator_unadjust) 
mediator_unadjust %>% as_kable()
```

Both tables together

```{r}
mediator_tbl <- tbl_merge(list(mediator_adjust, mediator_unadjust))
mediator_tbl %>% as_kable()
```

With a mediator we want to understand both the direct effect and the total effect. The direct effect is the direct association between the exposure and outcome independent of the mediator which we know is 1. The total effect is the effect of the exposure through the covariate to the outcome, which we know is 0. In this case to get the direct effect we __should__ adjust for the mediator, and to get the total effect we __should not__ adjust for the mediator. 

All this said, mediation is a complex topic on it's own and we will cover some of the more advanced analyses methods later in the course. The key point here is that you need to have a causal model based on the literature that a variable is a mediator. 

## M-Bias

### M-Bias DAG

```{r}
m_bias_dag <- m_bias(
  x = "Education",
  y = "Diabetes",
  a = "Income during Childhood",
  b = "Genetic Risk \nfor Diabetes",
  m = "Mother's Diabetes"
) 

ggdag(m_bias_dag) + theme_dag_blank()

ggdag(m_bias_dag, use_labels = "label") + theme_dag_blank()
```

From the `ggdag` package

> From a classical confounding perspective, it seems like the mother’s diabetes status might be a confounder: it’s associated with both the exposure and outcome, and it’s not a descendant of either. However, the association with the outcome and exposure is not direct for either; it’s due to confounding by genetic risk and childhood income, respectively. Drawing it as a DAG makes it clear that the mother’s diabetes status is a collider, and adjusting for it will induce an association between genetic risk and childhood income, thus opening a back-door path from education to diabetes status. [link](https://r-causal.github.io/ggdag/articles/bias-structures.html).

So based on what we know about m-bias so far should adjust or not adjust in this case? __NO__

Let's compare what happens when we do and do not adjust for the m-bias 

##### M-Bias Regression

Get the data

```{r}
m_bias_data <- filter(causal_quartet, dataset == "(4) M-Bias")
```

Adjusted model

```{r}
m_bias_adjust <- lm(outcome ~ exposure + covariate, data = m_bias_data)
m_bias_adjust <- tbl_regression(m_bias_adjust)
m_bias_adjust %>% as_kable()
```

Unadjusted model 

```{r}
m_bias_unadjust <- lm(outcome ~ exposure, data = m_bias_data)
m_bias_unadjust <- tbl_regression(m_bias_unadjust) 
m_bias_unadjust %>% as_kable()
```

Both tables together

```{r}
m_bias_tbl <- tbl_merge(list(m_bias_adjust, m_bias_unadjust))
m_bias_tbl %>% as_kable()
```

## Causal Quartet Conclusions

| Data generating mechanism | ATE not adjusting for pre-exposure | ATE adjusting for pre-exposure | Correct average causal effect |
| ---- | ---- | ---- | ---- | 
| (1) Collider | 1 | 1 | 1 |
| (2) Confounder | 1 | 0.5 | 0.5 |
| (3) Mediator | 1 | 1 | 1 |
| (4) M-Bias| 1 | 0.88 | 1 |

From the paper

> Here we have demonstrated that when presented with an exposure, outcome, and some measured factors, statistics alone, whether summary statistics or data visualizations, are insufficient to determine the appropriate causal estimate. Analysts need additional information about the data generating mechanism to draw the correct conclusions. While knowledge of the data generating process is necessary to estimate the correct causal effect in each of the cases presented, an analyst can take steps to make mistakes such as those shown here less likely. [link](https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2276446).

Remember at the beginning of class we talked about adjustment and how in general we should favour less statistical adjustment rather than more. The causal quartet is a toy example but you can see how if you have many many covariates the chance that you are create more bias than you remove is real, especially as your DAG gets more complex. 

# Rashomon's Quartet

This data from the paper [here](https://arxiv.org/abs/2302.13356). Data and producible example of the paper are [here](https://github.com/MI2DataLab/rashomon-quartet)

Inspired by Anscombe's quartet, this paper introduces a Rashomon Quartet, i.e. a set of four models built on a synthetic dataset which have practically identical predictive performance. However, the visual exploration reveals distinct explanations of the relations in the data. This illustrative example aims to encourage the use of methods for model visualization to compare predictive models beyond their performance. The name “Rashomon” refers to the title of a movie by Akira Kurosawa, in which four witnesses describe the same event in very different ways. 

![Rashomon Effect](causal_quartet_R_files/figure1.png)
Illustration of the Rashomon effect: equally effective models, each telling a different story. Dashed contours indicate an equal value of a loss function calculated on a validation dataset. Squares indicate “best in class” models trained on the same data. The Rashomon Quartet is designed so that the best models have an equal loss function on the validation data, but each model from the set describes a different perspective.

## Data

```{r}
train <- rashomon_quartet_train
test <- rashomon_quartet_test
```

### Tree based model

Note slightly different model training as they are not using tidymodels for this example. 

```{r}
set.seed(1568) 

model_dt <- ctree(y~., data = train, control = ctree_control(maxdepth = 3, minsplit = 250))
exp_dt <- DALEX::explain(model_dt, data = test[,-1], y = test[,1], 
                         verbose = FALSE, label="decision tree")
mp_dt <- model_performance(exp_dt)

mp_dt
```

#### Plot the decision tree

```{r}
plot(model_dt)
```

### Linear Model

```{r}
set.seed(1568) 

model_lm <- lm(y~., data = train)
exp_lm <- DALEX::explain(model_lm, data = test[,-1], y = test[,1], 
                         verbose = FALSE, label="linear regression")
mp_lm <- model_performance(exp_lm)

mp_lm

summary(model_lm)
```

### Random Forest

```{r}
set.seed(1568) 

model_rf <- randomForest(y~., data = train, ntree = 100)
exp_rf <- DALEX::explain(model_rf, data = test[,-1], y = test[,1], 
                         verbose = FALSE, label="random forest")
mp_rf <- model_performance(exp_rf)

mp_rf

summary(model_rf)
```

### Neural Net

```{r}
set.seed(1568) 

model_nn <- neuralnet(y~., data = train, hidden=c(8, 4), threshold=0.05)
exp_nn <- DALEX::explain(model_nn, data = test[,-1], y = test[,1], 
                        verbose = FALSE, label="neural network")
mp_nn <- model_performance(exp_nn)

mp_nn
```

#### Plot the neural net

```{r}
plot(model_nn)
```

### Combine model performance

```{r}
mp_all <- list(lm = mp_lm, dt = mp_dt, nn = mp_nn, rf = mp_rf)

mp_all
```

## Variable Importance

```{r}
imp_dt <- model_parts(exp_dt, N=NULL, B=1, type = "difference")
imp_lm <- model_parts(exp_lm, N=NULL, B=1, type = "difference")
imp_rf <- model_parts(exp_rf, N=NULL, B=1, type = "difference")
imp_nn <- model_parts(exp_nn, N=NULL, B=1, type = "difference")

plot(imp_dt, imp_nn, imp_rf, imp_lm)
```

### The stories

1. __Linear regression__: Analysis of the fitted linear model suggests that x1 is the most important variable. Its coefficient is more than three times larger than that of x2. The coefficient on x3 is the smallest and negative (though not statistically significant). A more detailed analysis of the residuals would suggest more flexible models, such as polynomial regression, but in order to see this, one cannot just look at performance but needs to make residual plots.
2. __Decision tree__: Analysis of the fitted tree model suggests that the only important variable is x1. This differs from the story told through the linear model, not only by the absence of x2, but also from different behavior on the extremes x1, where it plateaus. Why has this happened? In successive splits, the variables are chosen greedily, so it may happen that the effect of x2 is ignored because the tree will choose the correlated dominant x1, exhausting the splits available to the decision tree.
3. __Random forest__: All three variables are important, x1 twice as strong as x2 and x3. The shape and nature of the relationship for each variable is similarly positive. Why is x3 more prominent in this model fit? This is due to the random sampling of variables for each tree in the random forest fit. Because x3 is correlated with x1, it will contribute similarly to the response value.
4. __Neural network__: For a neural network, the variables x1 and x2 are equally important and have a monotonic effect on y. The effect of the x2 variable is stronger than in the other three model fits. Curiously, x3 has a non-monotonic relationship with y.

### Visual Summary

![Rashomon Effect](causal_quartet_R_files/figure2.png)


